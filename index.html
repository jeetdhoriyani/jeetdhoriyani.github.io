<!DOCTYPE html>
<html>
<head>
<title>Simon Batzner</title>
</head>
<body>

<h1>Simon Batzner</h1>

<img src="./jeet.jpeg" alt="Simon" width="300" height="200">

<h2>Contact</h2>

E-Mail: echo "ude.dravrah.g@renztab" | rev<br><br>
Simon Batzner<br>
Harvard University<br>
29 Oxford St Pierce Hall, Room 302<br>
Cambridge, MA 02138<br>

<h2>Research Interests</h2>

My interests lie at the intersection of Deep Learning and Physics. More specifically, I am interested in improving and accelerating Molecular Simulation with the help of Deep Learning. A core focus of my work has been on the role of symmetry. In particular, my Ph.D. work introduced E(3)-equivariant Machine Learning Interatomic Potentials and was the first to demonstrate the large improvements they can yield in sample efficiency, accuracy, and out-of-distribution generalization. More recently, my interests have focused on scalability, generalization to out-of-distribution data, and theory of equivariant neural networks. Most notably, our team was able to scale a state-of-the-art equivariant ML potential to >100,000,000 atoms on as little as 128 GPUs while maintaining high throughput. Our approach has focused heavily on both designing novel algorithms for potentials while also implementing them in fast, massively scalable, and easy-to-use software that is widely used by researchers around the globe. 

<h2>News</h2>
<ul>
  <li> 06/2022: I started an internship at Google Brain in SF, working with Ekin Dogus Cubuk. 
  <li> 05/2022: I won the Best Student Presentation Award in the ML Symposium at MRS '22. Thank you to the organizers for an excellent symposium </li>
  <li> 05/2022: We're organizing the Swiss Equivariant Learning Workshop from July 11th - 14th 2022 in Lausanne, sign up <a href=https://sites.google.com/mit.edu/swiss-equivariant-learning> here </a> by June 24th.
  <li> 05/2022: Preprint alert: We have posted a <a href=https://arxiv.org/abs/2205.06643> preprint</a> on a unifying theory of E(3)-equivariant interatomic potentials. Joint work with the group of Gábor Csányi as well as Christoph Ortner and Ralf Drautz.
  <li> 05/2022: Our NequIP paper is <a href=https://www.nature.com/articles/s41467-022-29939-5> now published in Nature Commmunications </a></li>
  <li> 05/2022: The Allegro code is now public <a href=https://github.com/mir-group/allegro> on our group's GitHub</a></li>
  <li> 04/2022: My undergraduate university hosted me on their <a href=https://open.spotify.com/episode/1Pt56yePjLFUwCNQ2uQ8dV?si=2ffacd290b64434b&nd=1> Made in Science podcast </a> </li>
  <li>04/2022: I will be giving an invited talk at EPFL in the Chemistry department </li>
  <li>04/2022: I will be giving an invited talk in the <a href=https://hannes-stark.com/logag-reading-group> Logag reading group </a> covering the Allegro preprint. 
  <li>04/2022: Preprint alert: We have posted a <a href=https://arxiv.org/abs/2204.05249> preprint </a>  describing our new Machine Learning Interatomic Potential called Allegro
  <li>03/2022: Our work on <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.1c00143> multi-task learning of collective variables</a> was published in JCTC, led by Lixin Sun. 
  <li>03/2022: There are seven NequIP-related talks at APS March Meeting 2022 in Chicago, both from our group and others.
  <li>03/2022: I will be giving an invited symposium talk at <a href=https://meetings.aps.org/Meeting/MAR22/Session/Q15> APS March Meeting 2022 </a> in Chicago on Equivariant Interatomic Potentials</li>
  <li>01/2022: I will be joining the team at <a href=https://research.google/teams/brain/> Google Brain</a> for the summer of 2022. I look forward to working with <a href=https://scholar.google.com/citations?user=Mu_8iOEAAAAJ&hl=en> Ekin Dogus Cubuk</a>.</li>
  <li>12/2021: We have posted an updated version of our <a href=https://arxiv.org/abs/2101.03164> NequIP preprint</a></li>
  <li>12/2021: We have a tutorial and 4 accepted NequIP-related talks at MRS Fall 2021 in Boston!</li>
  <li>11/2021: We have released v0.5 of our <a href=https://github.com/mir-group/nequip> NequIP code</a></li>
</ul>

<hr>
<h2>About</h2>
<p>I am a third-year PhD student at Harvard University, fortunate to be advised by <a href="https://bkoz.seas.harvard.edu/">Boris Kozinsky</a>.<br><br>Prior to joining Harvard, I obtained a Master's from MIT, where I worked with Alexie Kolpak and Boris Kozinsky. At MIT, I also wrote a thesis on equivariant neural networks. Before that, I spent a year in Los Angeles, working on the NASA mission <a href="https://www.nasa.gov/mission_pages/SOFIA/index.html">SOFIA</a>, where I wrote software for analyzing telescope data and used ML to model the dynamics of piezolelectrics. I obtained my Bachelor's from the University of Stuttgart, Germany. I am originally from a <a href="https://en.wikipedia.org/wiki/Illertissen">small, but beautiful town a few minutes from the Bavarian Alps</a>.</p>

<h2>Education</h2>
<ul>
  <li>since 2019, Harvard University, Ph.D. candidate</li>
  <li>2017-2019, Massachusetts Institute of Technology, S.M.</li>
  <li>2016-2017, DSI at the NASA Armstrong Research Center, Thesis Candidate</li>
  <li>2013-2017, University of Stuttgart, Germany, B.Sc.</li>
</ul>

<hr>

<h2>Publications</h2>
<p>
Here is a link to my <a href="https://scholar.google.com/citations?user=364jgpgAAAAJ&hl=en">Google Scholar</a>. 

<h2>Invited talks</h2>
<p>
<ul>
  <li> 2022, EPFL, Institute of Chemical Sciences and Engineering
  <li> 2022, Learning on Graphs and Geometry Reading Group (<a href=https://www.youtube.com/watch?v=ZR1NTBPBDOo>recording </a> | <a href=https://hannes-stark.com/assets/2022_05_03_logag.pdf>slides</a>)</li>
  <li> 2022, APS March Meeting, Invited Symposium Speaker </li>
  <li>2021, Carnegie Mellon University, SciML Speaker Series (<a href="https://drive.google.com/file/d/1mlGTv2NyqoNRh7dYLz_p4v2nicXEymoL/view">slides</a>)</li>
  <li>2021, University of Minnesota</li>
  <li>2021, University of California at Berkeley, Teresa and Martin Head-Gordon groups</li>
  <li>2021, Comenius University, Bratislava, Slovakia, Martonak group</li>
  <li>2021, University of Cambridge, Machine Learning Discussion Group</li>
  <li>2021, MILA, Quebec AI Institue, LambdaZero team</li>
</ul>
</p>

<hr>

<h2>Software</h2>
We have published two codes for the NequIP and Allegro potential. Both are public on our group's Github. If you have questions, please reach out, we are happy to help: 

<ul>
  <li> <a href="https://github.com/mir-group/nequip"> NequIP</a>, MIT license </li>
  <li> <a href="https://github.com/mir-group/allegro"> Allegro</a>, MIT license </li>
</ul>

Both of these also come with a LAMMPS pair_style, which can be found here: 

<ul>
  <li> <a href="https://github.com/mir-group/pair_nequip"> pair_nequip</a>, MIT license </li>
  <li> <a href="https://github.com/mir-group/pair_allegro"> pair_allegro</a>, MIT license </li>
</ul>


<h2>Teaching</h2>
<p>
<ul>
  <li>Harvard AP 275, Spring 2021: Computational Design of Materials (TF)</li>
  <li>MIT 2.086, Spring 2019: Numerical Computation for Mechanical Engineers (TA)</li>
  <li>University of Stuttgart, 2014/2015: Higher Mathematics 1 + 2 (Tutor)</li>
</ul>
</p>

</body>
</html>
<hr>
Last updated: 08/16/2022
